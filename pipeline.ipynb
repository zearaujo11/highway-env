{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL for Autonomous Vehicle Tasks: Safety and Traffic Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define agents here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import highway_env\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lane Changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.6696173 ,  0.25      ,  0.3125    ,  0.        ],\n",
       "        [ 1.        ,  0.10771749,  0.25      , -0.04923636,  0.        ],\n",
       "        [ 1.        ,  0.20912096,  0.25      , -0.01369097,  0.        ],\n",
       "        [ 1.        ,  0.3164464 ,  0.5       , -0.02589392,  0.        ],\n",
       "        [ 1.        ,  0.4215841 ,  0.5       , -0.04881948,  0.        ]],\n",
       "       dtype=float32),\n",
       " {'speed': 25,\n",
       "  'crashed': False,\n",
       "  'action': 7,\n",
       "  'rewards': {'collision_reward': 0.0,\n",
       "   'right_lane_reward': 0.3333333333333333,\n",
       "   'high_speed_reward': 0.5,\n",
       "   'on_road_reward': 1.0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lane_changing_env = gymnasium.make('highway-v0', render_mode='rgb_array')\n",
    "config = {\n",
    "    \"vehicles_count\": 100,  # Number of vehicles in the environment\n",
    "    \"controlled_vehicles\": 1,  # Number of vehicles controlled by the agent\n",
    "    \"duration\": 30,  # Duration of each episode\n",
    "    #\"reverse_penalty\": 0,  # Large penalty for reversing\n",
    "    # \"drifting_penalty\": -1,  # Penalty for drifting or sharp turns\n",
    "    # \"off_road_penalty\": -1,  # Penalize for going off-road\n",
    "    \"collision_reward\": -1,  # Stronger penalty for collisions\n",
    "    \"lane_change_reward\": 0.1,  # Reward for staying in lane\n",
    "    \"right_lane_reward\": 0.2,  # Reward for being in the right lane\n",
    "    \"high_speed_reward\": 0.4,  # Reward for high speed\n",
    "    \"ego_spacing\": 1.5,  # Desired minimum spacing between the ego vehicle and the vehicle in front\n",
    "    \"reward_speed_range\": [20, 30],  # Reward only within this speed range\n",
    "    \"simulation_frequency\": 5,  # Lower simulation speed to avoid erratic behavior\n",
    "    \"initial_lane_id\": None,\n",
    "    \"policy_frequency\": 1,  # Fewer policy updates per second\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 400,\n",
    "    \"offscreen_rendering\": False,\n",
    "    \"offroad_terminal\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"show_trajectories\": True,\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteAction\"  # Discrete control (steer left, right, accelerate)\n",
    "    },\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "}\n",
    "\n",
    "lane_changing_env.unwrapped.configure(config)\n",
    "lane_changing_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roundabout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.0000000e+00,  2.0000000e-02,  4.4999999e-01,  0.0000000e+00,\n",
       "         -5.3333336e-01],\n",
       "        [ 1.0000000e+00, -2.4032215e-02,  1.9855088e-01,  1.0000000e+00,\n",
       "          1.5310200e-01],\n",
       "        [ 1.0000000e+00, -2.0917192e-01,  1.1767375e-01,  5.4431701e-01,\n",
       "          9.6755505e-01],\n",
       "        [ 1.0000000e+00,  1.0000000e+00, -2.0000000e-02, -1.0000000e+00,\n",
       "          2.2204460e-16],\n",
       "        [ 1.0000000e+00, -1.7556924e-01, -9.5788546e-02, -5.3577900e-01,\n",
       "          9.8202038e-01]], dtype=float32),\n",
       " {'speed': 8,\n",
       "  'crashed': False,\n",
       "  'action': 1,\n",
       "  'rewards': {'collision_reward': False,\n",
       "   'high_speed_reward': 0.0,\n",
       "   'lane_change_reward': False,\n",
       "   'on_road_reward': True}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roundabout_env = gymnasium.make('roundabout-v0', render_mode='rgb_array')\n",
    "\n",
    "roundabout_config = {\n",
    "    \"controlled_vehicles\": 1,  # One agent-controlled vehicle\n",
    "    \"duration\": 11,  # Duration of each episode\n",
    "    \"collision_reward\": -1,  # Strong penalty for collisions\n",
    "    \"high_speed_reward\": 0.2,  # Incentivize maintaining appropriate speed\n",
    "    \"lane_change_reward\": -0.05,  # Minor reward for lane changes\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 400,\n",
    "    \"offscreen_rendering\": False,\n",
    "    \"offroad_terminal\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"show_trajectories\": True,\n",
    "    \"ego_spacing\": 1.0,  # Minimum spacing between vehicles\n",
    "    \"incoming_vehicle_destination\": None,  # All vehicles depart from the same lane\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteAction\",  # Discrete control for ease of navigation\n",
    "        \"target_speeds\": [0, 8, 16],  # Two target speeds for the agent\n",
    "    },\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",  # Focus on vehicle dynamics\n",
    "        \"absolute\": True,  # Use absolute coordinates\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],  # Maximum visible range in meters\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-15, 15],  # Maximum velocity range in m/s\n",
    "            \"vy\": [-15, 15],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "roundabout_env.unwrapped.configure(roundabout_config)\n",
    "roundabout_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overtaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  0.6860827 ,  0.        ,  0.3125    ,  0.        ],\n",
       "        [ 1.        ,  0.11369688,  0.        , -0.01974093,  0.        ],\n",
       "        [ 1.        ,  0.22093678,  0.5       , -0.01566802,  0.        ],\n",
       "        [ 1.        ,  0.31923932,  0.75      , -0.02453135,  0.        ],\n",
       "        [ 1.        ,  0.42891288,  0.25      , -0.03623617,  0.        ]],\n",
       "       dtype=float32),\n",
       " {'speed': 25,\n",
       "  'crashed': False,\n",
       "  'action': 6,\n",
       "  'rewards': {'collision_reward': 0.0,\n",
       "   'right_lane_reward': 0.0,\n",
       "   'high_speed_reward': 0.5,\n",
       "   'on_road_reward': 1.0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overtaking_env = gymnasium.make('highway-v0', render_mode='rgb_array')\n",
    "config = {\n",
    "    \"vehicles_count\": 50,  # Number of vehicles in the environment\n",
    "    \"controlled_vehicles\": 1,  # Number of vehicles controlled by the agent\n",
    "    \"duration\": 30,  # Duration of each episode\n",
    "    \"collision_reward\": -1,  # Stronger penalty for collisions\n",
    "    \"lane_change_reward\": 0.5,  # Reward for staying in lane\n",
    "    \"right_lane_reward\": 0.1,  # Reward for being in the right lane\n",
    "    \"overtaking_reward\": 1,  # Reward for overtaking\n",
    "    \"reward_speed_range\": [20, 30],  # Reward only within this speed range\n",
    "    \"simulation_frequency\": 5,  # Lower simulation speed to avoid erratic behavior\n",
    "    \"policy_frequency\": 1,  # Fewer policy updates per second\n",
    "    \"ego_spacing\": 1.5,  # Desired minimum spacing between the ego vehicle and the vehicle in front\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 400,\n",
    "    \"offscreen_rendering\": False,\n",
    "    \"offroad_terminal\": True,\n",
    "    \"show_trajectories\": True,\n",
    "    \"render_agent\": True,\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteAction\"  # Discrete control (steer left, right, accelerate)\n",
    "    },\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\"\n",
    "    },\n",
    "}\n",
    "overtaking_env.unwrapped.configure(config)\n",
    "overtaking_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_directory(path):\n",
    "    \"\"\"\n",
    "    Create directory if it doesn't exist\n",
    "    \n",
    "    Args:\n",
    "        path (str): Directory path to create\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logger.info(f\"Directory created: {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating directory {path}: {e}\")\n",
    "\n",
    "def save_pipeline_data(agent_name, stage, environment_name, data):\n",
    "    \"\"\"\n",
    "    Save pipeline stage data\n",
    "    \n",
    "    Args:\n",
    "        agent_name (str): Name of the RL agent\n",
    "        stage (str): Current pipeline stage\n",
    "        environment_name (str): Name of the environment\n",
    "        data (dict): Data to be saved\n",
    "    \"\"\"\n",
    "    create_directory(f\"results/{agent_name}\")\n",
    "    \n",
    "    try:\n",
    "        filename = f\"results/{agent_name}/{environment_name}_{stage}_data.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        logger.info(f\"Data saved for {stage} in {environment_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, environment, agent_name, stage, environment_name, timesteps=1000):\n",
    "    \"\"\"\n",
    "    Train the agent in a specific environment and save the model\n",
    "    \n",
    "    Args:\n",
    "        agent (sb3.BaseAlgorithm): RL agent to train\n",
    "        environment (gym.Env): Environment to train in\n",
    "        agent_name (str): Name of the agent\n",
    "        stage (str): Current pipeline stage\n",
    "        environment_name (str): Name of the environment\n",
    "        timesteps (int, optional): Number of training timesteps. Defaults to 100.\n",
    "    \n",
    "    Returns:\n",
    "        Trained agent\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Wrap environment with Monitor\n",
    "        log_dir = f\"logs/{agent_name}\"\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        env = Monitor(environment, log_dir)\n",
    "            \n",
    "        # Set environment and learn\n",
    "        agent.set_env(env)\n",
    "        agent.learn(total_timesteps=timesteps)\n",
    "\n",
    "        # Get training results\n",
    "        training_results = {\n",
    "            'rewards': env.get_episode_rewards(),\n",
    "        }\n",
    "        \n",
    "        # Create directories for model and results\n",
    "        create_directory(f\"models/{agent_name}\")\n",
    "        \n",
    "        # Save model\n",
    "        agent.save(f\"models/{agent_name}/{environment_name}_model\")\n",
    "        \n",
    "        # Prepare and save training information\n",
    "        training_info = {\n",
    "            'agent_name': agent_name,\n",
    "            'environment_name': environment_name,\n",
    "            'stage': stage,\n",
    "            'total_timesteps': timesteps,\n",
    "            'training_results': training_results\n",
    "        }\n",
    "        save_pipeline_data(agent_name, \"train\", environment_name, training_info)\n",
    "        \n",
    "        logger.info(f\"Training completed for {agent_name} in {environment_name}\")\n",
    "        return agent\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed for {agent_name} in {environment_name}: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, environment, agent_name, stage, environment_name, num_episodes=10):\n",
    "    test_results = []\n",
    "    total_rewards = []\n",
    "    total_collisions = 0\n",
    "    traffic_speeds = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        try:\n",
    "            obs, info = environment.reset()\n",
    "            done = truncated = False\n",
    "            episode_reward = 0\n",
    "            episode_collisions = 0\n",
    "            episode_speeds = []\n",
    "\n",
    "            while not (done or truncated):\n",
    "                # Predict action using the agent\n",
    "                action, _states = agent.predict(obs, deterministic=True)\n",
    "                obs, reward, done, truncated, info = environment.step(action)\n",
    "\n",
    "                # Update reward\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # Check for collisions\n",
    "                if info.get(\"crashed\", False):\n",
    "                    episode_collisions += 1\n",
    "\n",
    "                # Gather traffic speeds\n",
    "                current_speeds = [\n",
    "                    vehicle.speed\n",
    "                    for vehicle in environment.unwrapped.road.vehicles  # Use unwrapped\n",
    "                    if vehicle != environment.unwrapped.vehicle  # Exclude agent vehicle\n",
    "                ]\n",
    "                episode_speeds.extend(current_speeds)\n",
    "\n",
    "                # Optional rendering\n",
    "                environment.render()\n",
    "\n",
    "            environment.close()\n",
    "\n",
    "            # Collect data for this episode\n",
    "            total_rewards.append(episode_reward)\n",
    "            total_collisions += episode_collisions\n",
    "            traffic_speeds.extend(episode_speeds)\n",
    "\n",
    "            test_results.append({\n",
    "                'episode': episode,\n",
    "                'total_reward': episode_reward,\n",
    "                'collisions': episode_collisions,\n",
    "                'avg_speed': np.mean(episode_speeds) if episode_speeds else 0,\n",
    "                'speed_variance': np.var(episode_speeds) if episode_speeds else 0\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Test episode {episode + 1} failed in {environment_name}: {e}\")\n",
    "\n",
    "    # Calculate overall KPIs\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_speed = np.mean(traffic_speeds) if traffic_speeds else 0\n",
    "    speed_variance = np.var(traffic_speeds) if traffic_speeds else 0\n",
    "\n",
    "    logger.info(f\"Test completed in {environment_name} - \"\n",
    "                f\"Avg Reward: {avg_reward}, Total Collisions: {total_collisions}, \"\n",
    "                f\"Avg Traffic Speed: {avg_speed}, Speed Variance: {speed_variance}\")\n",
    "\n",
    "    # Save test results and KPIs\n",
    "    save_pipeline_data(agent_name, stage, environment_name, {\n",
    "        'test_results': test_results,\n",
    "        'kpis': {\n",
    "            'average_reward': avg_reward,\n",
    "            'total_collisions': total_collisions,\n",
    "            'average_speed': avg_speed,\n",
    "            'speed_variance': speed_variance\n",
    "        },\n",
    "        'environment': environment_name,\n",
    "        'stage': stage\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        'test_results': test_results,\n",
    "        'kpis': {\n",
    "            'average_reward': avg_reward,\n",
    "            'total_collisions': total_collisions,\n",
    "            'average_speed': avg_speed,\n",
    "            'speed_variance': speed_variance\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_pipeline(initial_agent, agent_name, environments):\n",
    "    \"\"\"\n",
    "    Sequential environment training pipeline with customizable order\n",
    "    \n",
    "    Args:\n",
    "        initial_agent (sb3.BaseAlgorithm): Initial RL agent\n",
    "        agent_name (str): Name of the agent\n",
    "        environments (list of tuples): A list of (env_name, gym.Env) tuples defining the order of execution.\n",
    "            Example: [(\"lane_changing\", lane_changing_env), (\"roundabout\", roundabout_env), (\"overtaking\", overtaking_env)]\n",
    "    \n",
    "    Returns:\n",
    "        Trained agent\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Customizable Sequential Environment Pipeline Started\")\n",
    "    \n",
    "    current_agent = initial_agent\n",
    "\n",
    "    for i, (env_name, env) in enumerate(environments):\n",
    "        logger.info(f\"Stage {i + 1}: {env_name.capitalize()} Environment\")\n",
    "        \n",
    "        if i > 0:  # Test the previous agent in the current environment before training\n",
    "            previous_env_name = environments[i - 1][0]\n",
    "            logger.info(f\"Testing {previous_env_name.capitalize()} Agent in {env_name.capitalize()}\")\n",
    "            test(\n",
    "                current_agent, env, \n",
    "                agent_name, 'pre_training_test', env_name\n",
    "            )\n",
    "        \n",
    "        # Train in the current environment\n",
    "        logger.info(f\"Training in {env_name.capitalize()} Environment\")\n",
    "        current_agent = train(\n",
    "            current_agent, env, \n",
    "            agent_name, env_name, env_name, 20000\n",
    "        )\n",
    "        \n",
    "        # Test in the current environment\n",
    "        logger.info(f\"Testing in {env_name.capitalize()} Environment\")\n",
    "        test(\n",
    "            current_agent, env, \n",
    "            agent_name, 'pos_training_test', env_name\n",
    "        )\n",
    "\n",
    "    logger.info(\"Customizable Sequential Environment Pipeline Completed\")\n",
    "\n",
    "    # Save the final agent\n",
    "    create_directory(f\"models/{agent_name}/final\")\n",
    "    current_agent.save(f\"models/{agent_name}/{agent_name}_final_model\")\n",
    "    \n",
    "    return current_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir \"agent_DQN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Agent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 10:31:31,797 - INFO - Customizable Sequential Environment Pipeline Started\n",
      "2024-12-12 10:31:31,798 - INFO - Stage 1: Roundabout Environment\n",
      "2024-12-12 10:31:31,798 - INFO - Training in Roundabout Environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to agent_DQN/DQN_10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 3.07     |\n",
      "|    exploration_rate | 0.98     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 2.15     |\n",
      "|    exploration_rate | 0.959    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 86       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 2.12     |\n",
      "|    exploration_rate | 0.938    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 130      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.81     |\n",
      "|    ep_rew_mean      | 1.95     |\n",
      "|    exploration_rate | 0.925    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 157      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | 1.97     |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 201      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.83     |\n",
      "|    ep_rew_mean      | 1.74     |\n",
      "|    exploration_rate | 0.888    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 236      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0496   |\n",
      "|    n_updates        | 35       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10       |\n",
      "|    ep_rew_mean      | 1.93     |\n",
      "|    exploration_rate | 0.867    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 280      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0371   |\n",
      "|    n_updates        | 79       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.84     |\n",
      "|    ep_rew_mean      | 2.02     |\n",
      "|    exploration_rate | 0.85     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 315      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0798   |\n",
      "|    n_updates        | 114      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.97     |\n",
      "|    ep_rew_mean      | 2.05     |\n",
      "|    exploration_rate | 0.829    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 359      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.049    |\n",
      "|    n_updates        | 158      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.9      |\n",
      "|    ep_rew_mean      | 2.22     |\n",
      "|    exploration_rate | 0.812    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 396      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0461   |\n",
      "|    n_updates        | 195      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.64     |\n",
      "|    ep_rew_mean      | 2.24     |\n",
      "|    exploration_rate | 0.799    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 424      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0686   |\n",
      "|    n_updates        | 223      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.75     |\n",
      "|    ep_rew_mean      | 2.22     |\n",
      "|    exploration_rate | 0.778    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 468      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.08     |\n",
      "|    n_updates        | 267      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.83     |\n",
      "|    ep_rew_mean      | 2.35     |\n",
      "|    exploration_rate | 0.757    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 511      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.042    |\n",
      "|    n_updates        | 310      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.71     |\n",
      "|    ep_rew_mean      | 2.22     |\n",
      "|    exploration_rate | 0.742    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 544      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0939   |\n",
      "|    n_updates        | 343      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.8      |\n",
      "|    ep_rew_mean      | 2.23     |\n",
      "|    exploration_rate | 0.721    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 588      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0663   |\n",
      "|    n_updates        | 387      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.58     |\n",
      "|    ep_rew_mean      | 2.21     |\n",
      "|    exploration_rate | 0.709    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 613      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0532   |\n",
      "|    n_updates        | 412      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.62     |\n",
      "|    ep_rew_mean      | 2.2      |\n",
      "|    exploration_rate | 0.689    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 654      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0739   |\n",
      "|    n_updates        | 453      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.61     |\n",
      "|    ep_rew_mean      | 2.26     |\n",
      "|    exploration_rate | 0.671    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 692      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.057    |\n",
      "|    n_updates        | 491      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.57     |\n",
      "|    ep_rew_mean      | 2.19     |\n",
      "|    exploration_rate | 0.655    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 727      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0913   |\n",
      "|    n_updates        | 526      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.43     |\n",
      "|    ep_rew_mean      | 2.12     |\n",
      "|    exploration_rate | 0.642    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 754      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0483   |\n",
      "|    n_updates        | 553      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.5      |\n",
      "|    ep_rew_mean      | 2.2      |\n",
      "|    exploration_rate | 0.621    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 798      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0495   |\n",
      "|    n_updates        | 597      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.44     |\n",
      "|    ep_rew_mean      | 2.2      |\n",
      "|    exploration_rate | 0.605    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 831      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0681   |\n",
      "|    n_updates        | 630      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.45     |\n",
      "|    ep_rew_mean      | 2.18     |\n",
      "|    exploration_rate | 0.587    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 869      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0632   |\n",
      "|    n_updates        | 668      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.43     |\n",
      "|    ep_rew_mean      | 2.17     |\n",
      "|    exploration_rate | 0.57     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 905      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.063    |\n",
      "|    n_updates        | 704      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.4      |\n",
      "|    ep_rew_mean      | 2.12     |\n",
      "|    exploration_rate | 0.553    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 940      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0265   |\n",
      "|    n_updates        | 739      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.42     |\n",
      "|    ep_rew_mean      | 2.02     |\n",
      "|    exploration_rate | 0.533    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 984      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.065    |\n",
      "|    n_updates        | 783      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.31     |\n",
      "|    ep_rew_mean      | 2.01     |\n",
      "|    exploration_rate | 0.517    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 1017     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0759   |\n",
      "|    n_updates        | 816      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.24     |\n",
      "|    ep_rew_mean      | 2.07     |\n",
      "|    exploration_rate | 0.499    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total_timesteps  | 1054     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0606   |\n",
      "|    n_updates        | 853      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.32     |\n",
      "|    ep_rew_mean      | 2.04     |\n",
      "|    exploration_rate | 0.483    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 1089     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0461   |\n",
      "|    n_updates        | 888      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.23     |\n",
      "|    ep_rew_mean      | 2.06     |\n",
      "|    exploration_rate | 0.466    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 1124     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0519   |\n",
      "|    n_updates        | 923      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.32     |\n",
      "|    ep_rew_mean      | 2.09     |\n",
      "|    exploration_rate | 0.445    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 86       |\n",
      "|    total_timesteps  | 1168     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0559   |\n",
      "|    n_updates        | 967      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.16     |\n",
      "|    ep_rew_mean      | 2.04     |\n",
      "|    exploration_rate | 0.432    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total_timesteps  | 1196     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0554   |\n",
      "|    n_updates        | 995      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.08     |\n",
      "|    ep_rew_mean      | 2.06     |\n",
      "|    exploration_rate | 0.419    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 1223     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0504   |\n",
      "|    n_updates        | 1022     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.91     |\n",
      "|    ep_rew_mean      | 2.03     |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 1250     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0869   |\n",
      "|    n_updates        | 1049     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.73     |\n",
      "|    ep_rew_mean      | 1.95     |\n",
      "|    exploration_rate | 0.397    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total_timesteps  | 1269     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0709   |\n",
      "|    n_updates        | 1068     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.72     |\n",
      "|    ep_rew_mean      | 1.98     |\n",
      "|    exploration_rate | 0.384    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total_timesteps  | 1296     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.113    |\n",
      "|    n_updates        | 1095     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.46     |\n",
      "|    ep_rew_mean      | 1.98     |\n",
      "|    exploration_rate | 0.376    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 1314     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0756   |\n",
      "|    n_updates        | 1113     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.38     |\n",
      "|    ep_rew_mean      | 1.94     |\n",
      "|    exploration_rate | 0.359    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 1349     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.127    |\n",
      "|    n_updates        | 1148     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.22     |\n",
      "|    ep_rew_mean      | 1.95     |\n",
      "|    exploration_rate | 0.351    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total_timesteps  | 1366     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0548   |\n",
      "|    n_updates        | 1165     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.13     |\n",
      "|    ep_rew_mean      | 1.95     |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 104      |\n",
      "|    total_timesteps  | 1401     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.115    |\n",
      "|    n_updates        | 1200     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.24     |\n",
      "|    ep_rew_mean      | 1.97     |\n",
      "|    exploration_rate | 0.317    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 107      |\n",
      "|    total_timesteps  | 1437     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0372   |\n",
      "|    n_updates        | 1236     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.1      |\n",
      "|    ep_rew_mean      | 1.97     |\n",
      "|    exploration_rate | 0.305    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 1464     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0714   |\n",
      "|    n_updates        | 1263     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.99     |\n",
      "|    ep_rew_mean      | 1.91     |\n",
      "|    exploration_rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 111      |\n",
      "|    total_timesteps  | 1491     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.146    |\n",
      "|    n_updates        | 1290     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.92     |\n",
      "|    ep_rew_mean      | 2.01     |\n",
      "|    exploration_rate | 0.278    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 1519     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0845   |\n",
      "|    n_updates        | 1318     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8        |\n",
      "|    ep_rew_mean      | 2.04     |\n",
      "|    exploration_rate | 0.262    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 1554     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.051    |\n",
      "|    n_updates        | 1353     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.74     |\n",
      "|    ep_rew_mean      | 2        |\n",
      "|    exploration_rate | 0.253    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total_timesteps  | 1572     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0552   |\n",
      "|    n_updates        | 1371     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.85     |\n",
      "|    ep_rew_mean      | 2.06     |\n",
      "|    exploration_rate | 0.232    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 120      |\n",
      "|    total_timesteps  | 1616     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0618   |\n",
      "|    n_updates        | 1415     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.83     |\n",
      "|    ep_rew_mean      | 2.1      |\n",
      "|    exploration_rate | 0.215    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 123      |\n",
      "|    total_timesteps  | 1652     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0517   |\n",
      "|    n_updates        | 1451     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.65     |\n",
      "|    ep_rew_mean      | 2.1      |\n",
      "|    exploration_rate | 0.207    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 125      |\n",
      "|    total_timesteps  | 1670     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.111    |\n",
      "|    n_updates        | 1469     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.55     |\n",
      "|    ep_rew_mean      | 2.17     |\n",
      "|    exploration_rate | 0.195    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total_timesteps  | 1695     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0291   |\n",
      "|    n_updates        | 1494     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.26     |\n",
      "|    ep_rew_mean      | 2.22     |\n",
      "|    exploration_rate | 0.188    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 1710     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0514   |\n",
      "|    n_updates        | 1509     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.19     |\n",
      "|    ep_rew_mean      | 2.24     |\n",
      "|    exploration_rate | 0.175    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total_timesteps  | 1736     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0893   |\n",
      "|    n_updates        | 1535     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.97     |\n",
      "|    ep_rew_mean      | 2.19     |\n",
      "|    exploration_rate | 0.168    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 131      |\n",
      "|    total_timesteps  | 1751     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0614   |\n",
      "|    n_updates        | 1550     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.89     |\n",
      "|    ep_rew_mean      | 2.23     |\n",
      "|    exploration_rate | 0.155    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 133      |\n",
      "|    total_timesteps  | 1778     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0702   |\n",
      "|    n_updates        | 1577     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8      |\n",
      "|    ep_rew_mean      | 2.24     |\n",
      "|    exploration_rate | 0.143    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 1804     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0891   |\n",
      "|    n_updates        | 1603     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.73     |\n",
      "|    ep_rew_mean      | 2.33     |\n",
      "|    exploration_rate | 0.126    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total_timesteps  | 1841     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0485   |\n",
      "|    n_updates        | 1640     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.8      |\n",
      "|    ep_rew_mean      | 2.38     |\n",
      "|    exploration_rate | 0.109    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 142      |\n",
      "|    total_timesteps  | 1876     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0561   |\n",
      "|    n_updates        | 1675     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.88     |\n",
      "|    ep_rew_mean      | 2.42     |\n",
      "|    exploration_rate | 0.0923   |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total_timesteps  | 1911     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0585   |\n",
      "|    n_updates        | 1710     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.05     |\n",
      "|    ep_rew_mean      | 2.56     |\n",
      "|    exploration_rate | 0.0714   |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 148      |\n",
      "|    total_timesteps  | 1955     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 1754     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.3      |\n",
      "|    ep_rew_mean      | 2.76     |\n",
      "|    exploration_rate | 0.0505   |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 151      |\n",
      "|    total_timesteps  | 1999     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0889   |\n",
      "|    n_updates        | 1798     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.47     |\n",
      "|    ep_rew_mean      | 2.82     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 154      |\n",
      "|    total_timesteps  | 2043     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0892   |\n",
      "|    n_updates        | 1842     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.64     |\n",
      "|    ep_rew_mean      | 2.89     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total_timesteps  | 2078     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.105    |\n",
      "|    n_updates        | 1877     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.67     |\n",
      "|    ep_rew_mean      | 2.97     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 160      |\n",
      "|    total_timesteps  | 2116     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.189    |\n",
      "|    n_updates        | 1915     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.93     |\n",
      "|    ep_rew_mean      | 3.11     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total_timesteps  | 2159     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 1958     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.02     |\n",
      "|    ep_rew_mean      | 3.2      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total_timesteps  | 2203     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.138    |\n",
      "|    n_updates        | 2002     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.03     |\n",
      "|    ep_rew_mean      | 3.27     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total_timesteps  | 2240     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0649   |\n",
      "|    n_updates        | 2039     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.2      |\n",
      "|    ep_rew_mean      | 3.43     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 173      |\n",
      "|    total_timesteps  | 2284     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.108    |\n",
      "|    n_updates        | 2083     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.37     |\n",
      "|    ep_rew_mean      | 3.63     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 176      |\n",
      "|    total_timesteps  | 2328     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0841   |\n",
      "|    n_updates        | 2127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.53     |\n",
      "|    ep_rew_mean      | 3.73     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total_timesteps  | 2372     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0543   |\n",
      "|    n_updates        | 2171     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.62     |\n",
      "|    ep_rew_mean      | 3.91     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 183      |\n",
      "|    total_timesteps  | 2416     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.134    |\n",
      "|    n_updates        | 2215     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.88     |\n",
      "|    ep_rew_mean      | 4.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 186      |\n",
      "|    total_timesteps  | 2460     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 2259     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.88     |\n",
      "|    ep_rew_mean      | 4.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 189      |\n",
      "|    total_timesteps  | 2504     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0681   |\n",
      "|    n_updates        | 2303     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.96     |\n",
      "|    ep_rew_mean      | 4.26     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 193      |\n",
      "|    total_timesteps  | 2548     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.124    |\n",
      "|    n_updates        | 2347     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.22     |\n",
      "|    ep_rew_mean      | 4.44     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 196      |\n",
      "|    total_timesteps  | 2592     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.135    |\n",
      "|    n_updates        | 2391     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.4      |\n",
      "|    ep_rew_mean      | 4.48     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total_timesteps  | 2635     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.111    |\n",
      "|    n_updates        | 2434     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.67     |\n",
      "|    ep_rew_mean      | 4.64     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 203      |\n",
      "|    total_timesteps  | 2677     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0699   |\n",
      "|    n_updates        | 2476     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.85     |\n",
      "|    ep_rew_mean      | 4.8      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 206      |\n",
      "|    total_timesteps  | 2721     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0549   |\n",
      "|    n_updates        | 2520     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.1     |\n",
      "|    ep_rew_mean      | 4.93     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 209      |\n",
      "|    total_timesteps  | 2765     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 2564     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.3     |\n",
      "|    ep_rew_mean      | 5.17     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 2809     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0624   |\n",
      "|    n_updates        | 2608     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 5.4      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 215      |\n",
      "|    total_timesteps  | 2853     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.16     |\n",
      "|    n_updates        | 2652     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.5     |\n",
      "|    ep_rew_mean      | 5.51     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 2895     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0959   |\n",
      "|    n_updates        | 2694     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.6     |\n",
      "|    ep_rew_mean      | 5.65     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 222      |\n",
      "|    total_timesteps  | 2939     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0835   |\n",
      "|    n_updates        | 2738     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 5.85     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 225      |\n",
      "|    total_timesteps  | 2983     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.172    |\n",
      "|    n_updates        | 2782     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 6        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total_timesteps  | 3027     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.127    |\n",
      "|    n_updates        | 2826     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 6.07     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 232      |\n",
      "|    total_timesteps  | 3071     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0867   |\n",
      "|    n_updates        | 2870     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 6.19     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 235      |\n",
      "|    total_timesteps  | 3115     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0764   |\n",
      "|    n_updates        | 2914     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 6.39     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total_timesteps  | 3159     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.1      |\n",
      "|    n_updates        | 2958     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 6.56     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 241      |\n",
      "|    total_timesteps  | 3203     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.039    |\n",
      "|    n_updates        | 3002     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 6.59     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total_timesteps  | 3247     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.042    |\n",
      "|    n_updates        | 3046     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 6.71     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 248      |\n",
      "|    total_timesteps  | 3291     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.102    |\n",
      "|    n_updates        | 3090     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 6.9      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 251      |\n",
      "|    total_timesteps  | 3335     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0758   |\n",
      "|    n_updates        | 3134     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 6.93     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 3379     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0588   |\n",
      "|    n_updates        | 3178     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 258      |\n",
      "|    total_timesteps  | 3423     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.097    |\n",
      "|    n_updates        | 3222     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.14     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total_timesteps  | 3467     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 3266     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.25     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 264      |\n",
      "|    total_timesteps  | 3511     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.176    |\n",
      "|    n_updates        | 3310     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.33     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 267      |\n",
      "|    total_timesteps  | 3549     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.11     |\n",
      "|    n_updates        | 3348     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.41     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 271      |\n",
      "|    total_timesteps  | 3593     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0669   |\n",
      "|    n_updates        | 3392     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.48     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 274      |\n",
      "|    total_timesteps  | 3637     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0567   |\n",
      "|    n_updates        | 3436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.47     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 277      |\n",
      "|    total_timesteps  | 3673     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 3472     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.67     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 280      |\n",
      "|    total_timesteps  | 3717     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0553   |\n",
      "|    n_updates        | 3516     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.76     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 283      |\n",
      "|    total_timesteps  | 3761     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 3560     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.89     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 287      |\n",
      "|    total_timesteps  | 3805     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0611   |\n",
      "|    n_updates        | 3604     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 290      |\n",
      "|    total_timesteps  | 3849     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.116    |\n",
      "|    n_updates        | 3648     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.97     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 293      |\n",
      "|    total_timesteps  | 3893     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0702   |\n",
      "|    n_updates        | 3692     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.9      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 297      |\n",
      "|    total_timesteps  | 3937     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0595   |\n",
      "|    n_updates        | 3736     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 8.01     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 300      |\n",
      "|    total_timesteps  | 3981     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0703   |\n",
      "|    n_updates        | 3780     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 8.08     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 303      |\n",
      "|    total_timesteps  | 4025     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 3824     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 8.09     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 306      |\n",
      "|    total_timesteps  | 4069     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0586   |\n",
      "|    n_updates        | 3868     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.96     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 309      |\n",
      "|    total_timesteps  | 4106     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0337   |\n",
      "|    n_updates        | 3905     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.91     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 312      |\n",
      "|    total_timesteps  | 4150     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0624   |\n",
      "|    n_updates        | 3949     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.92     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 316      |\n",
      "|    total_timesteps  | 4194     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0973   |\n",
      "|    n_updates        | 3993     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.94     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 319      |\n",
      "|    total_timesteps  | 4238     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0732   |\n",
      "|    n_updates        | 4037     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.88     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 322      |\n",
      "|    total_timesteps  | 4282     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0494   |\n",
      "|    n_updates        | 4081     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 8.04     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 326      |\n",
      "|    total_timesteps  | 4326     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0806   |\n",
      "|    n_updates        | 4125     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 8        |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 328      |\n",
      "|    total_timesteps  | 4363     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0489   |\n",
      "|    n_updates        | 4162     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 7.96     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 332      |\n",
      "|    total_timesteps  | 4407     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0432   |\n",
      "|    n_updates        | 4206     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 8.03     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 335      |\n",
      "|    total_timesteps  | 4451     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.179    |\n",
      "|    n_updates        | 4250     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 7.87     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 338      |\n",
      "|    total_timesteps  | 4495     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.145    |\n",
      "|    n_updates        | 4294     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 7.8      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 341      |\n",
      "|    total_timesteps  | 4539     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0804   |\n",
      "|    n_updates        | 4338     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 7.7      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 345      |\n",
      "|    total_timesteps  | 4583     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0393   |\n",
      "|    n_updates        | 4382     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.75     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 348      |\n",
      "|    total_timesteps  | 4627     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0521   |\n",
      "|    n_updates        | 4426     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.79     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 351      |\n",
      "|    total_timesteps  | 4671     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0761   |\n",
      "|    n_updates        | 4470     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.8      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 355      |\n",
      "|    total_timesteps  | 4715     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0935   |\n",
      "|    n_updates        | 4514     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.93     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 358      |\n",
      "|    total_timesteps  | 4759     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 4558     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.87     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 361      |\n",
      "|    total_timesteps  | 4803     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.073    |\n",
      "|    n_updates        | 4602     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.89     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 364      |\n",
      "|    total_timesteps  | 4847     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.177    |\n",
      "|    n_updates        | 4646     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.82     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 367      |\n",
      "|    total_timesteps  | 4891     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0983   |\n",
      "|    n_updates        | 4690     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.82     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 371      |\n",
      "|    total_timesteps  | 4935     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0664   |\n",
      "|    n_updates        | 4734     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.81     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 374      |\n",
      "|    total_timesteps  | 4979     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0831   |\n",
      "|    n_updates        | 4778     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.79     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 377      |\n",
      "|    total_timesteps  | 5023     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0513   |\n",
      "|    n_updates        | 4822     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.74     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 381      |\n",
      "|    total_timesteps  | 5066     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0645   |\n",
      "|    n_updates        | 4865     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.67     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 384      |\n",
      "|    total_timesteps  | 5110     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0477   |\n",
      "|    n_updates        | 4909     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.63     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 387      |\n",
      "|    total_timesteps  | 5154     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0507   |\n",
      "|    n_updates        | 4953     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.71     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 391      |\n",
      "|    total_timesteps  | 5198     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.199    |\n",
      "|    n_updates        | 4997     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.73     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 394      |\n",
      "|    total_timesteps  | 5242     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0328   |\n",
      "|    n_updates        | 5041     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.65     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 397      |\n",
      "|    total_timesteps  | 5286     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0944   |\n",
      "|    n_updates        | 5085     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.65     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 400      |\n",
      "|    total_timesteps  | 5330     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.139    |\n",
      "|    n_updates        | 5129     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.66     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 403      |\n",
      "|    total_timesteps  | 5374     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.106    |\n",
      "|    n_updates        | 5173     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.54     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 407      |\n",
      "|    total_timesteps  | 5418     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0611   |\n",
      "|    n_updates        | 5217     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | 7.64     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 410      |\n",
      "|    total_timesteps  | 5462     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0544   |\n",
      "|    n_updates        | 5261     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | 7.59     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 413      |\n",
      "|    total_timesteps  | 5506     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0645   |\n",
      "|    n_updates        | 5305     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | 7.45     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 417      |\n",
      "|    total_timesteps  | 5550     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0737   |\n",
      "|    n_updates        | 5349     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | 7.55     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 420      |\n",
      "|    total_timesteps  | 5594     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0754   |\n",
      "|    n_updates        | 5393     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11       |\n",
      "|    ep_rew_mean      | 7.56     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 423      |\n",
      "|    total_timesteps  | 5638     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0669   |\n",
      "|    n_updates        | 5437     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.59     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 426      |\n",
      "|    total_timesteps  | 5674     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.052    |\n",
      "|    n_updates        | 5473     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.57     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 429      |\n",
      "|    total_timesteps  | 5718     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.142    |\n",
      "|    n_updates        | 5517     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.58     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 433      |\n",
      "|    total_timesteps  | 5762     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0527   |\n",
      "|    n_updates        | 5561     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.53     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 436      |\n",
      "|    total_timesteps  | 5806     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0423   |\n",
      "|    n_updates        | 5605     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.49     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 439      |\n",
      "|    total_timesteps  | 5850     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0961   |\n",
      "|    n_updates        | 5649     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.5      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 442      |\n",
      "|    total_timesteps  | 5894     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0455   |\n",
      "|    n_updates        | 5693     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.48     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 446      |\n",
      "|    total_timesteps  | 5938     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0613   |\n",
      "|    n_updates        | 5737     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 7.5      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 449      |\n",
      "|    total_timesteps  | 5982     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0856   |\n",
      "|    n_updates        | 5781     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.4      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 452      |\n",
      "|    total_timesteps  | 6018     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.156    |\n",
      "|    n_updates        | 5817     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.47     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 455      |\n",
      "|    total_timesteps  | 6062     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0621   |\n",
      "|    n_updates        | 5861     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.42     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 458      |\n",
      "|    total_timesteps  | 6106     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0273   |\n",
      "|    n_updates        | 5905     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.46     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 462      |\n",
      "|    total_timesteps  | 6150     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0394   |\n",
      "|    n_updates        | 5949     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.52     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 465      |\n",
      "|    total_timesteps  | 6194     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.071    |\n",
      "|    n_updates        | 5993     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.49     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 468      |\n",
      "|    total_timesteps  | 6238     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0358   |\n",
      "|    n_updates        | 6037     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.5      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 471      |\n",
      "|    total_timesteps  | 6282     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.049    |\n",
      "|    n_updates        | 6081     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.52     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 475      |\n",
      "|    total_timesteps  | 6326     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0495   |\n",
      "|    n_updates        | 6125     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.63     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 478      |\n",
      "|    total_timesteps  | 6370     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0611   |\n",
      "|    n_updates        | 6169     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.59     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 481      |\n",
      "|    total_timesteps  | 6414     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0503   |\n",
      "|    n_updates        | 6213     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.56     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 484      |\n",
      "|    total_timesteps  | 6451     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0539   |\n",
      "|    n_updates        | 6250     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.55     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 487      |\n",
      "|    total_timesteps  | 6495     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0648   |\n",
      "|    n_updates        | 6294     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.53     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 491      |\n",
      "|    total_timesteps  | 6539     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0492   |\n",
      "|    n_updates        | 6338     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 7.5      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 13       |\n",
      "|    time_elapsed     | 494      |\n",
      "|    total_timesteps  | 6582     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0573   |\n",
      "|    n_updates        | 6381     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m agent_DQN \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, roundabout_env,\n\u001b[1;32m      2\u001b[0m         policy_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(net_arch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m]),\n\u001b[1;32m      3\u001b[0m         learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     12\u001b[0m         tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_DQN/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m final_agent_DQN \u001b[38;5;241m=\u001b[39m \u001b[43mrl_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_DQN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent_DQN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroundabout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroundabout_env\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#(\"lane_changing\", lane_changing_env), (\"roundabout\", roundabout_env), (\"overtaking\", overtaking_env)])\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\" Maybe compare to this! \"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# # Training order is lane changing -> roundabout -> overtaking\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# final_agent_DQN_lro = rl_pipeline(agent_DQN, 'agent_DQN_lro', [(\"lane_changing\", lane_changing_env), (\"roundabout\", roundabout_env), (\"overtaking\", overtaking_env)])\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# # Training order is lane changing -> overtaking -> roundabout\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# final_agent_DQN_lor = rl_pipeline(agent_DQN, 'agent_DQN_lor', [(\"lane_changing\", lane_changing_env), (\"overtaking\", overtaking_env), (\"roundabout\", roundabout_env)])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 32\u001b[0m, in \u001b[0;36mrl_pipeline\u001b[0;34m(initial_agent, agent_name, environments)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train in the current environment\u001b[39;00m\n\u001b[1;32m     31\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m current_agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Test in the current environment\u001b[39;00m\n\u001b[1;32m     38\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(agent, environment, agent_name, stage, environment_name, timesteps)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Set environment and learn\u001b[39;00m\n\u001b[1;32m     23\u001b[0m agent\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[0;32m---> 24\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Get training results\u001b[39;00m\n\u001b[1;32m     27\u001b[0m training_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m: env\u001b[38;5;241m.\u001b[39mget_episode_rewards(),\n\u001b[1;32m     29\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gymnasium/wrappers/common.py:408\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gymnasium/core.py:317\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    315\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gymnasium/wrappers/common.py:300\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/envs/common/abstract.py:242\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulate(action)\n\u001b[0;32m--> 242\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n\u001b[1;32m    244\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_terminated()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/envs/common/observation.py:251\u001b[0m, in \u001b[0;36mKinematicObservation.observe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m close_vehicles:\n\u001b[1;32m    249\u001b[0m     origin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserver_vehicle \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsolute \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     vehicles_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m--> 251\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserve_intentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve_intentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_vehicles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvehicles_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, vehicles_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/envs/common/observation.py:252\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m close_vehicles:\n\u001b[1;32m    249\u001b[0m     origin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserver_vehicle \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsolute \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     vehicles_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    251\u001b[0m         [\n\u001b[0;32m--> 252\u001b[0m             \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserve_intentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve_intentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m close_vehicles[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\n\u001b[1;32m    254\u001b[0m         ]\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, vehicles_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/vehicle/kinematics.py:245\u001b[0m, in \u001b[0;36mVehicle.to_dict\u001b[0;34m(self, origin_vehicle, observe_intentions)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m, origin_vehicle: Vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, observe_intentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    240\u001b[0m     d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvelocity\u001b[49m[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheading\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading,\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos_h\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin_h\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos_d\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdestination_direction[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin_d\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdestination_direction[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_off\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_offset[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat_off\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_offset[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mang_off\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_offset[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    254\u001b[0m     }\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m observe_intentions:\n\u001b[1;32m    256\u001b[0m         d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos_d\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin_d\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/vehicle/kinematics.py:203\u001b[0m, in \u001b[0;36mVehicle.velocity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvelocity\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/highway_env/vehicle/objects.py:160\u001b[0m, in \u001b[0;36mRoadObject.direction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m             d[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m origin_dict[key]\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdirection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading), np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheading)])\n\u001b[1;32m    164\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvelocity\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent_DQN = DQN('MlpPolicy', roundabout_env,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "        learning_rate = 1e-4,\n",
    "        buffer_size=15000,\n",
    "        learning_starts=200,\n",
    "        batch_size=32,\n",
    "        gamma=0.8,\n",
    "        train_freq=1,\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=50,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"agent_DQN/\")\n",
    "\n",
    "final_agent_DQN = rl_pipeline(agent_DQN, 'agent_DQN', [(\"roundabout\", roundabout_env)])#(\"lane_changing\", lane_changing_env), (\"roundabout\", roundabout_env), (\"overtaking\", overtaking_env)])\n",
    "\n",
    "\n",
    "\"\"\" Maybe compare to this! \"\"\"\n",
    "\n",
    "# # Training order is lane changing -> roundabout -> overtaking\n",
    "# final_agent_DQN_lro = rl_pipeline(agent_DQN, 'agent_DQN_lro', [(\"lane_changing\", lane_changing_env), (\"roundabout\", roundabout_env), (\"overtaking\", overtaking_env)])\n",
    "\n",
    "# # Training order is roundabout -> overtaking -> lane changing\n",
    "# final_agent_DQN_rol = rl_pipeline(agent_DQN, 'agent_DQN_rol', [(\"roundabout\", roundabout_env), (\"overtaking\", overtaking_env), (\"lane_changing\", lane_changing_env)])\n",
    "\n",
    "# # Training order is overtaking -> lane changing -> roundabout\n",
    "# final_agent_DQN_olr = rl_pipeline(agent_DQN, 'agent_DQN_olr', [(\"overtaking\", overtaking_env), (\"lane_changing\", lane_changing_env), (\"roundabout\", roundabout_env)])\n",
    "\n",
    "# # Training order is overtaking -> roundabout -> lane changing\n",
    "# final_agent_DQN_orl = rl_pipeline(agent_DQN, 'agent_DQN_orl', [(\"overtaking\", overtaking_env), (\"roundabout\", roundabout_env), (\"lane_changing\", lane_changing_env)])\n",
    "\n",
    "# # Training order is roundabout -> lane changing -> overtaking\n",
    "# final_agent_DQN_rlo = rl_pipeline(agent_DQN, 'agent_DQN_rlo', [(\"roundabout\", roundabout_env), (\"lane_changing\", lane_changing_env), (\"overtaking\", overtaking_env)])\n",
    "\n",
    "# # Training order is lane changing -> overtaking -> roundabout\n",
    "# final_agent_DQN_lor = rl_pipeline(agent_DQN, 'agent_DQN_lor', [(\"lane_changing\", lane_changing_env), (\"overtaking\", overtaking_env), (\"roundabout\", roundabout_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Agent N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "def display_results(agent_name, stages, type=\"table\"):\n",
    "    # Load results\n",
    "    results = {}\n",
    "    for stage in stages:\n",
    "        with open(f\"results/{agent_name}/{stage}_data.json\", 'r') as f:\n",
    "            results[stage] = json.load(f)\n",
    "\n",
    "    if type == \"list\":\n",
    "        print(f\"--- {agent_name} ---\\n\")\n",
    "        for stage, data in results.items():\n",
    "            print(f\"> {stage}\")\n",
    "            print(f\"    - Average Reward: {data['kpis']['average_reward']}\")\n",
    "            print(f\"    - Total Collisions: {data['kpis']['total_collisions']}\")\n",
    "            print(f\"    - Average Speed: {data['kpis']['average_speed']}\")\n",
    "            print(f\"    - Speed Variance: {data['kpis']['speed_variance']}\")\n",
    "            print(\"\")\n",
    "        print(\"--------------------\\n\")\n",
    "    elif type == \"table\":\n",
    "        table = []\n",
    "        for stage, data in results.items():\n",
    "            table.append([stage, data['kpis']['average_reward'], data['kpis']['total_collisions'], data['kpis']['average_speed'], data['kpis']['speed_variance']])\n",
    "        table = np.array(table)\n",
    "\n",
    "        print(\"|\".join([f\"{{[{str(agent_name)}]:<20}} {str('Stage'):>10} \", \"Average Reward\".center(20), \"Total Collisions\".center(20), \"Average Speed\".center(20), \"Speed Variance\".center(20)]))\n",
    "        print(\"=\" * 117)\n",
    "        for row in table:\n",
    "            formatted_row = [\n",
    "                f\"{str(row[0]):>31} \",    # Stage (left aligned)\n",
    "                f\"{str(row[1]):^20}\",    # Average Reward (center aligned)\n",
    "                f\"{str(row[2]):^20}\",    # Total Collisions (center aligned)\n",
    "                f\"{str(row[3]):^20}\",    # Average Speed (center aligned)\n",
    "                f\"{str(row[4]):^20}\",     # Speed Variance (center aligned)\n",
    "            ]\n",
    "            print(\"|\".join(formatted_row))\n",
    "            print(\"-\" * 117)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[agent_DQN]:<20}      Stage |   Average Reward   |  Total Collisions  |   Average Speed    |   Speed Variance   \n",
      "=====================================================================================================================\n",
      "lane_changing_pos_training_test | 7.683333333333333  |         10         | 20.38586665050951  | 2.994659433616237  \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "   roundabout_pre_training_test |        5.0         |         8          | 13.176681440042938 | 17.91196936083314  \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "   roundabout_pos_training_test | 8.633333333333333  |         0          | 15.363874398994303 | 3.696733004851535  \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "   overtaking_pre_training_test | 9.662684444444448  |         8          | 20.412763661430365 | 2.3286179404918492 \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "   overtaking_pos_training_test | 17.742222222222228 |         4          | 20.61790829643619  | 1.9296350508335416 \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_results(\"agent_DQN\", [\"lane_changing_pos_training_test\", \"roundabout_pre_training_test\", \"roundabout_pos_training_test\", \"overtaking_pre_training_test\", \"overtaking_pos_training_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final agent in all environments\n",
    "def test_agent_all_envs(agent_name, environments, environment_names):\n",
    "    # Load the final agent\n",
    "    agent = DQN.load(f\"models/{agent_name}/{agent_name}_final_model\", device=\"cpu\")\n",
    "\n",
    "    # Lane Changing\n",
    "    performance_lane_changing = test(agent, environments[0], agent_name, '', \"final_test_\" + environment_names[0])\n",
    "\n",
    "    # Roundabout\n",
    "    performance_roundabout = test(agent, environments[1], agent_name, '', \"final_test_\" + environment_names[1])\n",
    "\n",
    "    # Overtaking\n",
    "    performance_overtaking = test(agent, environments[2], agent_name, '', \"final_test_\" + environment_names[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 10:30:48,203 - INFO - Test completed in final_test_roundabout - Avg Reward: 7.712499999999999, Total Collisions: 1, Avg Traffic Speed: 15.013559340080562, Speed Variance: 13.78392022820183\n",
      "2024-12-12 10:30:48,204 - INFO - Directory created: results/agent_DQN\n",
      "2024-12-12 10:30:48,205 - INFO - Data saved for  in final_test_roundabout\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_agent_all_envs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magent_DQN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mroundabout_env\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroundabout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#[lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# test_agent_all_envs(\"agent_DQN_rol\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# test_agent_all_envs(\"agent_DQN_olr\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# test_agent_all_envs(\"agent_DQN_orl\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# test_agent_all_envs(\"agent_DQN_rlo\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# test_agent_all_envs(\"agent_DQN_lor\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m, in \u001b[0;36mtest_agent_all_envs\u001b[0;34m(agent_name, environments, environment_names)\u001b[0m\n\u001b[1;32m      7\u001b[0m performance_lane_changing \u001b[38;5;241m=\u001b[39m test(agent, environments[\u001b[38;5;241m0\u001b[39m], agent_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_test_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m environment_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Roundabout\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m performance_roundabout \u001b[38;5;241m=\u001b[39m test(agent, \u001b[43menvironments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, agent_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_test_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m environment_names[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Overtaking\u001b[39;00m\n\u001b[1;32m     13\u001b[0m performance_overtaking \u001b[38;5;241m=\u001b[39m test(agent, environments[\u001b[38;5;241m2\u001b[39m], agent_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_test_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m environment_names[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_agent_all_envs(\"agent_DQN\", [roundabout_env], [\"roundabout\"])#[lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\n",
    "\n",
    "\n",
    "# test_agent_all_envs(\"agent_DQN_rol\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\n",
    "# test_agent_all_envs(\"agent_DQN_olr\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\n",
    "# test_agent_all_envs(\"agent_DQN_orl\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\n",
    "# test_agent_all_envs(\"agent_DQN_rlo\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\n",
    "# test_agent_all_envs(\"agent_DQN_lor\", [lane_changing_env, roundabout_env, overtaking_env], [\"lane_changing\", \"roundabout\", \"overtaking\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{[agent_DQN]:<20}      Stage |   Average Reward   |  Total Collisions  |   Average Speed    |   Speed Variance   \n",
      "=====================================================================================================================\n",
      "      final_test_lane_changing_ |      16.4125       |         4          | 20.602228832201057 | 1.7628795927127507 \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "         final_test_roundabout_ |       12.05        |         0          | 14.381887780369855 | 19.82951714419477  \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "         final_test_overtaking_ | 19.708888888888897 |         3          | 20.647436386620896 | 1.9794265486352098 \n",
      "---------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display final test results\n",
    "display_results(\"agent_DQN\", [\"final_test_lane_changing_\", \"final_test_roundabout_\", \"final_test_overtaking_\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
